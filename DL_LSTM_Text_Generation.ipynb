{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL LSTM Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNR6d9AUImHMOd8YEVlfc59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/Data_Science_Notebooks/blob/main/DL_LSTM_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de texto sintético con una red LSTM \n",
        "\n",
        "Referencia: https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\n"
      ],
      "metadata": {
        "id": "Abew8EitiUux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pZJRGBSLiFu7"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import gensim\n",
        "import string\n",
        "\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Descargar archivo de abstracts de Stanford\n",
        "\n",
        "# Original\n",
        "# https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt\n",
        "\n",
        "# Local\n",
        "url = 'https://raw.githubusercontent.com/tozanni/Data_Science_Notebooks/main/arxiv_abstracts.txt'\n",
        "path = get_file('arxiv_abstracts.txt', origin=url)\n"
      ],
      "metadata": {
        "id": "2Qi4wyI8iie-"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generar sentencias de longitud 40\n",
        "max_sentence_len = 40\n",
        "with open(path) as file_:\n",
        "  docs = file_.readlines()\n",
        "  sentences = [[word for word in doc.lower().split()[:max_sentence_len]] for doc in docs]\n",
        "\n",
        "print('Num sentences:', len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpF1RMFDi360",
        "outputId": "71575cd0-04ba-4b1c-92d2-60db1700d543"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num sentences: 7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización con Word2Vec\n",
        "\n",
        "A continuación se entrenará el modelo de embeddings Word2Vec, dicho modelo nos permitirá representar nuestras palabras en vectores que mantienen ciertas propiedades de similaridad semántica en sus dimensiones."
      ],
      "metadata": {
        "id": "P3LD9hsrjZOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]\n",
        "\n",
        "print('Entrenando modelo word2vec con 100 dimensiones...')\n",
        "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "\n",
        "print('Obtener palabras similares a algunos ejemplos:')\n",
        "for word in ['model', 'network', 'train', 'learn']:\n",
        "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
        "  print('  %s -> %s' % (word, most_similar))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35NdZXljjSca",
        "outputId": "6ed548e0-bbd5-446d-dff0-30a8c4405646"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando word2vec...\n",
            "Result embedding shape: (1350, 100)\n",
            "Obtener palabras similares a algunos ejemplos:\n",
            "  model -> $l_p$ (0.41), technique (0.38), trains (0.34), 2012) (0.33), architecture. (0.33), continuous (0.31), al, (0.30), of (0.30)\n",
            "  network -> networks (0.33), constrained (0.32), architecture (0.24), there (0.24), trained (0.23), connected (0.23), by (0.23), guide (0.23)\n",
            "  train -> based (0.37), classical (0.35), eigendecompositions (0.33), sequentially (0.31), map (0.31), extend (0.30), average (0.30), then (0.30)\n",
            "  learn -> remain (0.37), automatically (0.36), lower (0.36), relevant (0.36), effort (0.36), effectively (0.34), upper (0.34), adopted (0.34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de training y test set para LSTM"
      ],
      "metadata": {
        "id": "pjRyfqEmkT91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  train_y[i] = word2idx(sentence[-1])\n",
        "\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)\n",
        "\n",
        "# Ejemplo de datos de training y test\n",
        "# Nuestras secuencias de training y test son los índices \n",
        "# de las palabras del diccionario\n",
        "\n",
        "train_x[0], train_y[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhUl2dL-iRrM",
        "outputId": "a8d38096-cab7-4f03-bd19-c851adf3d021"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x shape: (7200, 40)\n",
            "train_y shape: (7200,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  4, 463,   5, 464, 465, 130,   1, 104, 131,  26,  19, 170, 466,\n",
              "         46, 251,  11, 105, 252,  17,   2, 467, 253,   1, 171, 130, 254,\n",
              "        468, 469, 470, 172, 471, 472,  52, 473, 474, 475, 476,   2, 255,\n",
              "          0], dtype=int32), 3)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición y entrenamiento de la red LSTM\n",
        "\n",
        "1. Notar que la primera capa corresponde a la capa de embeddings, de tal forma que el primer proceso de la red será mapear el input al espacio de los embeddings.\n",
        "\n",
        "2. Posteriormente se aplica la capa LSTM con una cantidad de unidades arbitraria definida por nosotros. \n",
        "\n",
        "3. Finalmente, pasamos a una capa densa con tantas neuronas como palabras e nuestro vocabulario con activación Softmax para generar como output una distribución de probabilidades de la siguiente palabra."
      ],
      "metadata": {
        "id": "h_YAUyd2wgmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "\n",
        "model.add(LSTM(units=16))\n",
        "\n",
        "#Modelo con dos capas anidadas recurrentes\n",
        "#model.add(LSTM(units=32, return_sequences=True))\n",
        "#model.add(LSTM(units=16))\n",
        "\n",
        "#Probar otras definiciones de units\n",
        "#model.add(LSTM(units=emdedding_size))\n",
        "\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax')) #El resultado es un vector de probabilidades\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnMePvPDkgF7",
        "outputId": "03a88cd1-b97b-4a7c-c226-3c7fa3689126"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 6s 58ms/step - loss: 6.6278\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 4s 62ms/step - loss: 5.1156\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 4.2167\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 3s 59ms/step - loss: 3.6985\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 3s 59ms/step - loss: 3.3432\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.0883\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.8828\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.7091\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.5444\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.3909\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.2547\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.1308\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.0146\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 1.9060\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.8040\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.7031\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.5875\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.4599\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.3436\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 1.2376\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7874c55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto con el modelo LSTM\n",
        "\n",
        "Ahora hay que generar texto sintético, en esta etapa utilizaremos el modelo de forma iterativa comenzando por una semilla, posteriormente concatenando el output de cada etapa y pasandolo a la siguiente iteración.\n",
        "\n",
        "Notar que el modelo entrega una distribución de probabilidad de las siguientes palabras más probables y no es idóneo elegir siempre la mayor (ej. usando argmax) por lo cual se recomienda aplicar un método de sampling sobre dicha distribución.\n",
        "\n",
        "\n",
        "Referencia de sampling:\n",
        "\n",
        "https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f\n"
      ],
      "metadata": {
        "id": "0GWn6NXvqX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  \"\"\"\n",
        "  Metodo de muestreo aleatorio de siguiente palabra.\n",
        "  Toma como input la distribucion de probabilidad entregada por la red.\n",
        "  Con cierta proabilidad dependiendo de la temperatura produce la \n",
        "  siguiente palabra.\n",
        "  \"\"\"\n",
        "\n",
        "  if temperature <= 0:\n",
        "    return np.argmax(preds)\n",
        "  \n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "def generate_next(text, num_generated=10):\n",
        "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "  for i in range(num_generated):\n",
        "    \n",
        "    #El input se incrementa en cada iteracion a la RNN\n",
        "    print(\"Input --> \",word_idxs)\n",
        "    x=np.array(word_idxs)\n",
        "\n",
        "    #Tenemos que convertir el input a 3D agregando una dimension dummy\n",
        "    x=np.expand_dims(x,1) \n",
        "    prediction = model.predict(x)\n",
        "    print(\"Prediction -->\", prediction)\n",
        "\n",
        "    #No realizar sampling, tomar la palabra con mayor probabilidad\n",
        "    #idx = np.argmax(prediction[-1])\n",
        "\n",
        "    #Realizar un muestreo aleatorio\n",
        "    idx = sample(prediction[-1], temperature=0.7)\n",
        "\n",
        "    word_idxs.append(idx)\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)\n"
      ],
      "metadata": {
        "id": "cyChj7MykmT_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_words = 20 #Cuantas palabras se generaran?\n",
        "\n",
        "generated_text = generate_next('deep convolutional', next_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxtRN7pzmh4q",
        "outputId": "1674ece6-52ee-40fa-d1af-de38b7e0e69a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input -->  [6, 39]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184686 0.00290873 ... 0.00068148 0.00061358 0.00062416]]\n",
            "Input -->  [6, 39, 921]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]]\n",
            "Input -->  [6, 39, 921, 1232]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " [0.00041619 0.00226063 0.00072092 ... 0.00073284 0.0006961  0.0007712 ]]\n",
            "Input -->  [6, 39, 921, 1232, 934]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " [0.00041619 0.00226063 0.00072092 ... 0.00073284 0.0006961  0.0007712 ]\n",
            " [0.01290017 0.00240372 0.00791572 ... 0.00052954 0.00051007 0.00046165]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " [0.00041619 0.00226063 0.00072092 ... 0.00073284 0.0006961  0.0007712 ]\n",
            " [0.01290017 0.00240372 0.00791572 ... 0.00052954 0.00051007 0.00046165]\n",
            " [0.01003709 0.00202847 0.00509273 ... 0.00057628 0.00062171 0.00058369]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.01290017 0.00240372 0.00791572 ... 0.00052954 0.00051007 0.00046165]\n",
            " [0.01003709 0.00202847 0.00509273 ... 0.00057628 0.00062171 0.00058369]\n",
            " [0.00307159 0.00244325 0.00288536 ... 0.00060407 0.00053575 0.00059042]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.01003709 0.00202847 0.00509273 ... 0.00057628 0.00062171 0.00058369]\n",
            " [0.00307159 0.00244325 0.00288536 ... 0.00060407 0.00053575 0.00059042]\n",
            " [0.00464191 0.00072292 0.00248552 ... 0.0006398  0.00065139 0.00061042]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00307159 0.00244325 0.00288536 ... 0.00060407 0.00053575 0.00059042]\n",
            " [0.00464191 0.00072292 0.00248552 ... 0.0006398  0.00065139 0.00061042]\n",
            " [0.00117153 0.0013465  0.00095308 ... 0.00071458 0.00071783 0.00072447]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00464192 0.00072292 0.00248552 ... 0.0006398  0.00065139 0.00061042]\n",
            " [0.00117153 0.0013465  0.00095308 ... 0.00071458 0.00071783 0.00072447]\n",
            " [0.0029461  0.00220088 0.00363439 ... 0.00060831 0.00063952 0.00063374]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00117153 0.0013465  0.00095308 ... 0.00071458 0.00071783 0.00072447]\n",
            " [0.0029461  0.00220088 0.00363439 ... 0.00060831 0.00063952 0.00063374]\n",
            " [0.00261924 0.00183488 0.00239724 ... 0.00066642 0.00065028 0.00070729]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.0029461  0.00220088 0.00363439 ... 0.00060831 0.00063952 0.00063374]\n",
            " [0.00261924 0.00183488 0.00239724 ... 0.00066642 0.00065028 0.00070729]\n",
            " [0.00332837 0.00258118 0.00272786 ... 0.00052508 0.00056628 0.00050065]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00261924 0.00183488 0.00239724 ... 0.00066642 0.00065028 0.00070729]\n",
            " [0.00332837 0.00258118 0.00272786 ... 0.00052508 0.00056628 0.00050065]\n",
            " [0.00675604 0.00339888 0.01319115 ... 0.00050576 0.0004796  0.00045874]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00332837 0.00258118 0.00272786 ... 0.00052508 0.00056628 0.00050065]\n",
            " [0.00675604 0.00339888 0.01319115 ... 0.00050576 0.0004796  0.00045874]\n",
            " [0.00065736 0.00141154 0.00066703 ... 0.00073617 0.00067346 0.00076229]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00675604 0.00339888 0.01319115 ... 0.00050576 0.0004796  0.00045874]\n",
            " [0.00065736 0.00141154 0.00066703 ... 0.00073618 0.00067346 0.00076229]\n",
            " [0.0005993  0.00071194 0.00028792 ... 0.00077589 0.00070925 0.00078047]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00065736 0.00141154 0.00066703 ... 0.00073618 0.00067346 0.00076229]\n",
            " [0.0005993  0.00071194 0.00028792 ... 0.00077589 0.00070925 0.00078047]\n",
            " [0.00149466 0.00222348 0.00106289 ... 0.00069376 0.00072475 0.00066128]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225, 826]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.0005993  0.00071194 0.00028792 ... 0.00077589 0.00070925 0.00078047]\n",
            " [0.00149466 0.00222348 0.00106289 ... 0.00069376 0.00072475 0.00066128]\n",
            " [0.0030498  0.00076663 0.00233964 ... 0.00065    0.00070094 0.00065452]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225, 826, 901]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00149466 0.00222348 0.00106289 ... 0.00069376 0.00072475 0.00066128]\n",
            " [0.0030498  0.00076663 0.00233964 ... 0.00065    0.00070094 0.00065452]\n",
            " [0.00292463 0.00130906 0.00137487 ... 0.00065279 0.00071664 0.00068038]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225, 826, 901, 1233]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.0030498  0.00076663 0.00233964 ... 0.00065    0.00070094 0.00065452]\n",
            " [0.00292463 0.00130906 0.00137487 ... 0.00065279 0.00071664 0.00068038]\n",
            " [0.00071981 0.00352869 0.00126143 ... 0.00068879 0.00064764 0.00073601]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225, 826, 901, 1233, 7]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00292463 0.00130906 0.00137487 ... 0.00065279 0.00071664 0.00068038]\n",
            " [0.00071981 0.00352869 0.00126143 ... 0.00068879 0.00064764 0.00073601]\n",
            " [0.00422011 0.01780517 0.03034961 ... 0.00022436 0.00020539 0.00021282]]\n",
            "Input -->  [6, 39, 921, 1232, 934, 450, 222, 268, 720, 576, 1110, 427, 1004, 878, 911, 1225, 826, 901, 1233, 7, 1178]\n",
            "Prediction --> [[0.00861872 0.00202773 0.0015632  ... 0.00059012 0.00065767 0.00066706]\n",
            " [0.00242453 0.00184685 0.00290873 ... 0.00068148 0.00061358 0.00062416]\n",
            " [0.00291707 0.00139112 0.00195958 ... 0.0006797  0.00065688 0.00068072]\n",
            " ...\n",
            " [0.00071981 0.00352869 0.00126143 ... 0.00068879 0.00064764 0.00073601]\n",
            " [0.00422011 0.01780516 0.03034962 ... 0.00022436 0.00020539 0.00021282]\n",
            " [0.00447505 0.00262318 0.00272499 ... 0.00060884 0.00064988 0.0006507 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#El texto final generado\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "G2NvWiPEoAxa",
        "outputId": "3fe8446e-d234-45a2-a235-e3ae710bb712"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deep convolutional post-synaptic gramian classical general efficient area (e.g., contrast recognized scaling investigation machines storage dominated sequence motivated angular neural dbn rate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios\n",
        "\n",
        "A. Realiza las siguientes modificaciones a la red y comenta los efectos que percibes en a) Valor de pérdida en las épocas. b) Tiempo de entrenamiento.\n",
        "c) Calidad percibida del texto final generado.\n",
        "\n",
        "1. Modifica la longitud de las secuencias de input.\n",
        "\n",
        "2. Modifica la cantidad de unidades de la capa LSTM.\n",
        "\n",
        "3. Modifica la cantidad de épocas de entrenamiento.\n",
        "\n",
        "4. Modifica la temperatura de sampling.\n",
        "\n",
        "5. Agrega una segunda capa recurrente LSTM (en ese caso la primer capa debe tener el parámetro return_sequences=True) ¿Percibes mejoras en relación a simplemente aumentar el tamaño de la capa?\n",
        "\n",
        "B. Presenta 3 ejemplos de texto generado por tu red. Puedes utilizar diferentes palabras de inicialización. "
      ],
      "metadata": {
        "id": "Aph7BIPhvNg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cOrIAsmjrC3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}