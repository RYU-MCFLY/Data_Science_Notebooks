{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL LSTM Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeWy/GhlcrUFBp7JmAKdsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/Data_Science_Notebooks/blob/main/DL_LSTM_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de texto sintético con una red LSTM \n",
        "\n",
        "Referencia: https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b\n"
      ],
      "metadata": {
        "id": "Abew8EitiUux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pZJRGBSLiFu7"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import gensim\n",
        "import string\n",
        "\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Descargar archivo de abstracts de Stanford\n",
        "\n",
        "# Original\n",
        "# https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt\n",
        "\n",
        "# Local\n",
        "url = 'https://raw.githubusercontent.com/tozanni/Data_Science_Notebooks/main/arxiv_abstracts.txt'\n",
        "path = get_file('arxiv_abstracts.txt', origin=url)\n"
      ],
      "metadata": {
        "id": "2Qi4wyI8iie-"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generar sentencias de longitud 40\n",
        "max_sentence_len = 40\n",
        "with open(path) as file_:\n",
        "  docs = file_.readlines()\n",
        "  sentences = [[word for word in doc.lower().split()[:max_sentence_len]] for doc in docs]\n",
        "\n",
        "print('Num sentences:', len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpF1RMFDi360",
        "outputId": "71575cd0-04ba-4b1c-92d2-60db1700d543"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num sentences: 7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización con Word2Vec\n",
        "\n",
        "A continuación se entrenará el modelo de embeddings Word2Vec, dicho modelo nos permitirá representar nuestras palabras en vectores que mantienen ciertas propiedades de similaridad semántica en sus dimensiones."
      ],
      "metadata": {
        "id": "P3LD9hsrjZOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]\n",
        "\n",
        "print('Entrenando modelo word2vec con 100 dimensiones...')\n",
        "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "\n",
        "print('Obtener palabras similares a algunos ejemplos:')\n",
        "for word in ['model', 'network', 'train', 'learn']:\n",
        "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
        "  print('  %s -> %s' % (word, most_similar))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35NdZXljjSca",
        "outputId": "6ed548e0-bbd5-446d-dff0-30a8c4405646"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando word2vec...\n",
            "Result embedding shape: (1350, 100)\n",
            "Obtener palabras similares a algunos ejemplos:\n",
            "  model -> $l_p$ (0.41), technique (0.38), trains (0.34), 2012) (0.33), architecture. (0.33), continuous (0.31), al, (0.30), of (0.30)\n",
            "  network -> networks (0.33), constrained (0.32), architecture (0.24), there (0.24), trained (0.23), connected (0.23), by (0.23), guide (0.23)\n",
            "  train -> based (0.37), classical (0.35), eigendecompositions (0.33), sequentially (0.31), map (0.31), extend (0.30), average (0.30), then (0.30)\n",
            "  learn -> remain (0.37), automatically (0.36), lower (0.36), relevant (0.36), effort (0.36), effectively (0.34), upper (0.34), adopted (0.34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de training y test set para LSTM"
      ],
      "metadata": {
        "id": "pjRyfqEmkT91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  train_y[i] = word2idx(sentence[-1])\n",
        "\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)\n",
        "\n",
        "# Ejemplo de datos de training y test\n",
        "# Nuestras secuencias de training y test son los índices \n",
        "# de las palabras del diccionario\n",
        "\n",
        "train_x[0], train_y[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhUl2dL-iRrM",
        "outputId": "a8d38096-cab7-4f03-bd19-c851adf3d021"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x shape: (7200, 40)\n",
            "train_y shape: (7200,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  4, 463,   5, 464, 465, 130,   1, 104, 131,  26,  19, 170, 466,\n",
              "         46, 251,  11, 105, 252,  17,   2, 467, 253,   1, 171, 130, 254,\n",
              "        468, 469, 470, 172, 471, 472,  52, 473, 474, 475, 476,   2, 255,\n",
              "          0], dtype=int32), 3)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfwW2GPBueng",
        "outputId": "e1a991c2-a672-444a-c17e-6aa7593e8234"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "\n",
        "model.add(LSTM(units=16))\n",
        "\n",
        "#model.add(LSTM(units=emdedding_size))\n",
        "\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax')) #El resultado es un vector de probabilidades\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnMePvPDkgF7",
        "outputId": "a5caf14a-89f1-4c74-f76a-c4cef857a0aa"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM...\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 8s 58ms/step - loss: 6.6431\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 5.1574\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 4.2703\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.8740\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.6320\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.4229\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.2232\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 3.0399\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.8763\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.7215\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.5823\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.4512\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 2.2986\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.1489\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 2.0068\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.8778\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 4s 63ms/step - loss: 1.7629\n",
            "Epoch 18/20\n",
            "57/57 [==============================] - 4s 67ms/step - loss: 1.6557\n",
            "Epoch 19/20\n",
            "57/57 [==============================] - 3s 58ms/step - loss: 1.5536\n",
            "Epoch 20/20\n",
            "57/57 [==============================] - 3s 57ms/step - loss: 1.4452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd787441190>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "  \"\"\"\n",
        "  Metodo de muestreo aleatorio de siguiente palabra.\n",
        "  Toma como input la distribucion de probabilidad entregada por la red.\n",
        "  Con cierta proabilidad dependiendo de la temperatura produce la \n",
        "  siguiente palabra.\n",
        "  \"\"\"\n",
        "\n",
        "  if temperature <= 0:\n",
        "    return np.argmax(preds)\n",
        "  \n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "def generate_next(text, num_generated=10):\n",
        "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "  for i in range(num_generated):\n",
        "    \n",
        "    #El input se incrementa en cada iteracion a la RNN\n",
        "    print(\"Input --> \",word_idxs)\n",
        "    x=np.array(word_idxs)\n",
        "\n",
        "    #Tenemos que convertir el input a 3D agregando una dimension dummy\n",
        "    x=np.expand_dims(x,1) \n",
        "    prediction = model.predict(x)\n",
        "    print(\"Prediction -->\", prediction)\n",
        "\n",
        "    #No realizar sampling, tomar la palabra con mayor probabilidad\n",
        "    #idx = np.argmax(prediction[-1])\n",
        "\n",
        "    #Realizar un muestreo aleatorio\n",
        "    idx = sample(prediction[-1], temperature=0.7)\n",
        "\n",
        "    word_idxs.append(idx)\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)\n"
      ],
      "metadata": {
        "id": "cyChj7MykmT_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto con el modelo LSTM\n",
        "\n",
        "Ahora hay que generar texto sintético, en esta etapa utilizaremos el modelo de forma iterativa comenzando por una semilla, posteriormente concatenando el output de cada etapa y pasandolo a la siguiente iteración.\n",
        "\n",
        "Notar que el modelo entrega una distribución de probabilidad de las siguientes palabras más probables y no es idóneo elegir siempre la mayor (ej. usando argmax) por lo cual se recomienda aplicar un método de sampling sobre dicha distribución.\n",
        "\n",
        "\n",
        "Referencia de sampling:\n",
        "\n",
        "https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f\n"
      ],
      "metadata": {
        "id": "0GWn6NXvqX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_words = 20 #Cuantas palabras se generaran?\n",
        "\n",
        "generated_text = generate_next('deep convolutional', next_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxtRN7pzmh4q",
        "outputId": "286a83e8-d892-4d4f-da0d-dddf67d5da25"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input -->  [6, 39]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]]\n",
            "Input -->  [6, 39, 217]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.00771011 0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]]\n",
            "Input -->  [6, 39, 217, 1031]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " [0.00431753 0.00186989 0.00351952 ... 0.00063165 0.00062848 0.00065779]]\n",
            "Input -->  [6, 39, 217, 1031, 541]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " [0.00431753 0.00186989 0.00351952 ... 0.00063165 0.00062848 0.00065779]\n",
            " [0.00055236 0.00264355 0.00056191 ... 0.00075236 0.00073002 0.00068422]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " [0.00431753 0.00186989 0.00351952 ... 0.00063165 0.00062848 0.00065779]\n",
            " [0.00055236 0.00264355 0.00056191 ... 0.00075236 0.00073002 0.00068422]\n",
            " [0.00131079 0.00150336 0.00128406 ... 0.00069368 0.00070988 0.00069699]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00055236 0.00264355 0.00056191 ... 0.00075236 0.00073002 0.00068422]\n",
            " [0.00131079 0.00150336 0.00128406 ... 0.00069368 0.00070988 0.00069699]\n",
            " [0.00250487 0.00223406 0.00218851 ... 0.0006162  0.00064799 0.00065276]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00131079 0.00150336 0.00128406 ... 0.00069368 0.00070988 0.00069699]\n",
            " [0.00250487 0.00223406 0.00218851 ... 0.0006162  0.00064799 0.00065276]\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00250487 0.00223406 0.00218851 ... 0.0006162  0.00064799 0.00065276]\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]\n",
            " [0.00168116 0.00121057 0.0013679  ... 0.00065323 0.00067761 0.00068681]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]\n",
            " [0.00168116 0.00121057 0.0013679  ... 0.00065323 0.00067761 0.00068681]\n",
            " [0.01016377 0.00581652 0.00813348 ... 0.00049156 0.00045687 0.00048461]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00168116 0.00121057 0.0013679  ... 0.00065323 0.00067761 0.00068681]\n",
            " [0.01016377 0.00581652 0.00813348 ... 0.00049156 0.00045687 0.00048461]\n",
            " [0.00206818 0.00471915 0.00192418 ... 0.00064357 0.00064335 0.00063052]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.01016377 0.00581652 0.00813348 ... 0.00049156 0.00045687 0.00048461]\n",
            " [0.00206818 0.00471915 0.00192418 ... 0.00064357 0.00064335 0.00063052]\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00206818 0.00471915 0.00192418 ... 0.00064357 0.00064335 0.00063052]\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]\n",
            " [0.0009143  0.00095206 0.00090174 ... 0.00073403 0.00073012 0.00071655]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00727448 0.00434856 0.0052318  ... 0.00058717 0.00054188 0.00060564]\n",
            " [0.0009143  0.00095206 0.00090174 ... 0.00073403 0.00073012 0.00071655]\n",
            " [0.00203968 0.0023809  0.00193443 ... 0.00067743 0.00065958 0.0006628 ]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.0009143  0.00095206 0.00090174 ... 0.00073403 0.00073012 0.00071655]\n",
            " [0.00203968 0.0023809  0.00193443 ... 0.00067743 0.00065958 0.0006628 ]\n",
            " [0.00418439 0.00127502 0.00323625 ... 0.00063592 0.00063617 0.00068321]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00203968 0.0023809  0.00193443 ... 0.00067743 0.00065958 0.0006628 ]\n",
            " [0.00418439 0.00127502 0.00323625 ... 0.00063592 0.00063617 0.00068321]\n",
            " [0.00139905 0.00118817 0.00120945 ... 0.00064589 0.00068965 0.00073815]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42, 677]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00418439 0.00127502 0.00323625 ... 0.00063592 0.00063617 0.00068321]\n",
            " [0.00139905 0.00118817 0.00120945 ... 0.00064589 0.00068965 0.00073815]\n",
            " [0.01078541 0.00313623 0.00825817 ... 0.00051061 0.00047213 0.00051589]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42, 677, 498]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00139905 0.00118817 0.00120945 ... 0.00064589 0.00068965 0.00073815]\n",
            " [0.01078541 0.00313623 0.00825817 ... 0.00051061 0.00047213 0.00051589]\n",
            " [0.00226709 0.00163663 0.00196427 ... 0.00065785 0.00063395 0.0006468 ]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42, 677, 498, 484]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.01078541 0.00313623 0.00825817 ... 0.00051061 0.00047213 0.00051589]\n",
            " [0.00226709 0.00163663 0.00196427 ... 0.00065785 0.00063395 0.0006468 ]\n",
            " [0.00047735 0.00042031 0.00046654 ... 0.00073823 0.00077199 0.00069943]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42, 677, 498, 484, 833]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00226709 0.00163663 0.00196427 ... 0.00065785 0.00063395 0.0006468 ]\n",
            " [0.00047735 0.00042031 0.00046654 ... 0.00073823 0.00077199 0.00069943]\n",
            " [0.01084071 0.00350157 0.007828   ... 0.0005602  0.00050529 0.00056399]]\n",
            "Input -->  [6, 39, 217, 1031, 541, 399, 583, 531, 693, 208, 17, 531, 1089, 862, 1015, 42, 677, 498, 484, 833, 563]\n",
            "Prediction --> [[0.01332337 0.00259415 0.00946838 ... 0.00039923 0.00042926 0.00047216]\n",
            " [0.00293603 0.00158464 0.00262481 ... 0.000575   0.00059777 0.0006546 ]\n",
            " [0.0077101  0.0045076  0.00603357 ... 0.00050402 0.00047231 0.00053058]\n",
            " ...\n",
            " [0.00047735 0.00042031 0.00046654 ... 0.00073823 0.00077199 0.00069943]\n",
            " [0.01084071 0.00350157 0.00782801 ... 0.0005602  0.00050529 0.00056399]\n",
            " [0.00377599 0.00222527 0.00316371 ... 0.00067665 0.00061998 0.00064386]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#El texto final generado\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "G2NvWiPEoAxa",
        "outputId": "b9190c93-e8c4-4c16-e3f4-cad559e04929"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deep convolutional algorithms. compiler speeding problems. represent speed methods, weights by speed coupled notion lot which lower-bounded restarting descent) (ii) achievable way'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios\n",
        "\n",
        "A. Realiza las siguientes modificaciones a la red y comenta los efectos que percibes en a) Valor de pérdida en las épocas. b) Tiempo de entrenamiento.\n",
        "c) Calidad percibida del texto final generado.\n",
        "\n",
        "1. Modifica la longitud de las secuencias de input.\n",
        "\n",
        "2. Modifica la cantidad de unidades de la capa LSTM.\n",
        "\n",
        "3. Modifica la cantidad de épocas de entrenamiento.\n",
        "\n",
        "4. Modifica la temperatura de sampling.\n",
        "\n",
        "B. Presenta 3 ejemplos de texto generado por tu red. Puedes utilizar diferentes palabras de inicialización. "
      ],
      "metadata": {
        "id": "Aph7BIPhvNg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cOrIAsmjrC3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}